## Attention Mechanism
To solve the bottleneck issue, the **Attention mechanism** was introduced. Instead of relying on a single context vector, Attention assigns different weights to different parts of the input sequence, allowing the decoder to focus on relevant words at each step.
- enabling the decoder to look at all encoder outputs (Weighted).
- Reduces the reliance on a single context vector.

**Benefits of Attention**
- **Improves performance on long sequences** by dynamically selecting relevant parts of the input.
-  **Eliminates the fixed-size bottleneck** by allowing the decoder to access all hidden states of the encoder.

![image](https://github.com/user-attachments/assets/78f2ca58-ddb5-4d22-9a82-4b95f37f6cb0)

![image](https://github.com/user-attachments/assets/9332af03-e0dd-48a3-ae13-dbdd7d8942f4)

### Attention Block Calculations
                                                  
- Inputs : (S(i), h1,h2,h3,.....,hn)                          
- Output : S(i)~                     
                                 
1. Calc Score                                      
Score (a,b) = a.b or f(W.a + W.b)                      
        - Score (s0, h1) = α1                       
	- Score (s0, h2) = α2                                
	- Score (s0, h3) = α3                                 
2. Soft max over Scores                          
	- (α1 + α2 + α3 = 1)                           
3. context vector =>>                    
	- c(0) = α1.h1 + α2.h2 + α3.h3            
5. Combine Context and Decoder State             
	- s(0)~ = tanh (s0, c0)                   
- α(i) = Score (s0, hi)               
- softmax (α)                      
- C(0) = SUM (α(i).h(i))                      
- s(0)~ = tanh (s0, c0)             
