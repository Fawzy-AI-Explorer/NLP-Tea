# Self Attention 

## Goal
-  Each Word computes a new Embedding by attending to all other Wordss, weighted similarity.
-  to compute a weighted representation of a sequence by allowing each token to focus on ("attend to") other tokens in the sequence.
In other words:                             
-  "How much should this word pay attention to other words ?"          


## How 

### Replace RNNs with Attention Blocks
-  In traditional RNNs, we process words one by one.
-  In self-attention, we process all words at once, using attention blocks instead of RNN cells.
   
-  For a sentence with 4 words â†’ we use 4 attention blocks (one per word).
-  Each block
  -  Input   : word embedding Xi 
  -  outputs : new Wrd embeding Yi
### Each Block = One Word's Attention Processing
-  Each word Xi updates itself by looking at other words in the sentence and deciding how each one are important.
![image](https://github.com/user-attachments/assets/ad44df6e-b54c-40f8-b349-f5b3ab84ae6a)


---
### HOW
e.g. we are on block 2, So Embedding of X2 Will Update        
1.  Similarity (Attention Scores)
  -  W21 = cos Sim (X1,X2) = X1.X2 / |X1|*|X2|     ,,,Range [-1, +1]                      
  -  W22 = cos Sim (X2,X2) = X2.X2 / |X2|*|X2|     ,,,will be max number X2 is Similar to X2    
  -  W23 = cos Sim (X3,X2) = X3.X2 / |X3|*|X2|
  -  W24 = cos Sim (X4,X2) = X4.X2 / |X4|*|X2|

scores SHowing how each Word Similar to X2    
scores showing how much attention X2 should give to each word           
COS Similarity Range from [1-, +1] Bounded But i want Probability Sum to ONE, So i will apply Sofe Max     

-  Softmax (1,2,3,4)     = 0.03, 0.08, 0.23, 0.64 
-  Softmax (5,10,15,20)  = 0, 0, 0.0067, 0.99
-  Softmax (10,20,30,40) = 0, 0, 0, 1                     
We Want to Normalize the Scores                                      

![image](https://github.com/user-attachments/assets/4d87739a-567e-41fc-9cc1-3d5e9b5fd6bc)


2. Normalize the Scores
  -  W2j = W2j / SQRT(d)
  -  d => Dim of Embedding NOT seq len 
3.   Softmax of Scores
  -  Softmax (W2j)         Range [0, +1]
  -  e.g. W25 => tells us how X2, X5 are Similar and how much weight to assign to X5

![image](https://github.com/user-attachments/assets/2afdfb71-8811-4dff-ad2f-970a5c4a4075)


4.  Weighted Sum = New Word Embedding
  -  Y2 = SUM (W2j * Xj)  
the new representation for word 2 is a weighted SUM of all the words in the sentence, based on how much attention it gave to each one.

![image](https://github.com/user-attachments/assets/83e5b92e-446a-4c0c-951c-b1f452d1d129)





