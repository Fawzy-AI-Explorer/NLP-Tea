# Self Attention 

## Goal
-  Each Word computes a new Embedding by attending to all other Wordss, weighted similarity.
-  to compute a weighted representation of a sequence by allowing each token to focus on ("attend to") other tokens in the sequence.
In other words:                             
-  "How much should this word pay attention to other words ?"          


## How 

### Replace RNNs with Attention Blocks
-  In traditional RNNs, we process words one by one.
-  In self-attention, we process all words at once, using attention blocks instead of RNN cells.
   
-  For a sentence with 4 words → we use 4 attention blocks (one per word).
-  Each block
  -  Input   : word embedding Xi 
  -  outputs : new Wrd embeding Yi
### Each Block = One Word's Attention Processing
-  Each word Xi updates itself by looking at other words in the sentence and deciding how each one are important.
![image](https://github.com/user-attachments/assets/ad44df6e-b54c-40f8-b349-f5b3ab84ae6a)


---
### HOW
e.g. we are on block 2, So Embedding of X2 Will Update        
1.  Similarity (Attention Scores)
  -  W21 = cos Sim (X1,X2) = X1.X2 / |X1|*|X2|     ,,,Range [-1, +1]                      
  -  W22 = cos Sim (X2,X2) = X2.X2 / |X2|*|X2|     ,,,will be max number X2 is Similar to X2    
  -  W23 = cos Sim (X3,X2) = X3.X2 / |X3|*|X2|
  -  W24 = cos Sim (X4,X2) = X4.X2 / |X4|*|X2|

scores SHowing how each Word Similar to X2    
scores showing how much attention X2 should give to each word           
COS Similarity Range from [1-, +1] Bounded But i want Probability Sum to ONE, So i will apply Sofe Max     

-  Softmax (1,2,3,4)     = 0.03, 0.08, 0.23, 0.64 
-  Softmax (5,10,15,20)  = 0, 0, 0.0067, 0.99
-  Softmax (10,20,30,40) = 0, 0, 0, 1                     
We Want to Normalize the Scores                                      

![image](https://github.com/user-attachments/assets/4d87739a-567e-41fc-9cc1-3d5e9b5fd6bc)


2. Normalize the Scores
  -  W2j = W2j / SQRT(d)
  -  d => Dim of Embedding NOT seq len 
3.   Softmax of Scores
  -  Softmax (W2j)         Range [0, +1]
  -  e.g. W25 => tells us how X2, X5 are Similar and how much weight to assign to X5

![image](https://github.com/user-attachments/assets/2afdfb71-8811-4dff-ad2f-970a5c4a4075)


4.  Weighted Sum = New Word Embedding
  -  Y2 = SUM (W2j * Xj)  
the new representation for word 2 is a weighted SUM of all the words in the sentence, based on how much attention it gave to each one.

![image](https://github.com/user-attachments/assets/83e5b92e-446a-4c0c-951c-b1f452d1d129)


```
Wij = Xi.T * Xj / |Xi| * |Xj|
```
The attention score between word i and word j is the dot product between their embeddings.       
This works well — but we want more flexibility.          
```
Wij = Xi.T * Xj / |Xi| * |Xj|
Wij = Wij / SQRT (d)
Wij = Softmax (Wij)
Yi = SUM (Wij) * Xj
```
![image](https://github.com/user-attachments/assets/2d706875-8b55-438a-bd8e-769d68726ac4)
```
select salary from t
      where Age = 20   >>>>>>>>>> 2000

select salary from t
      where Age = 35   >>>>>>>>>> Not Found
what if there is away to find it
Similarity between Query and All Keys to find Age 35 similar to each keys
Age = 35 , it it between (Similar) Age = 20 , 35   
We can say Salary = 0.5 * 2000 + 0.5 * 20000
Salary = 0*200 + 0.5*2000 + 0.5*20000 + 0*200 = SUM (W*V) => Weighted Sum of values 
```
-------------
-------------

-   Query ===>
   -   X we Work on it (Xi) (X2) Will Update
-   Keys ====>
   -   Wij = Xi.T * Xj / |Xi| * |Xj|  Here Xj are the Keys 
-   Value ===>
   -   Y2 = SUM (W2j * Xj)  Here Xj are the Values


Old
```
Wij = Xi.T * Xj / |Xi| * |Xj|
Wij = Wij / SQRT (d)
Wij = Softmax (Wij)
Yi = SUM (Wij) * Xj
```
New
```
Q = Wq * Xi
K = Wq * Xj
V = Wq * Xj

Wij = Q.T * K / |Q| * |K|
Wij = Wij / SQRT (d)
Wij = Softmax (Wij)
Yi = SUM (Wij) * V
```
![image](https://github.com/user-attachments/assets/4f1b890d-7cc7-4b07-aa5c-a051d124a3b2)
