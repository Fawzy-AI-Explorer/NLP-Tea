{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4382364",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167272d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\htc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\htc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import contractions\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d651715",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc53d23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>compliment_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AGNUgVwnZUey3gcPCJ76iw</td>\n",
       "      <td>3uLgwr0qeCNMjKenHJwPGQ</td>\n",
       "      <td>Avengers time with the ladies.</td>\n",
       "      <td>2012-05-18 02:17:21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NBN4MgHP9D3cw--SnauTkA</td>\n",
       "      <td>QoezRbYQncpRqyrLH6Iqjg</td>\n",
       "      <td>They have lots of good deserts and tasty cuban...</td>\n",
       "      <td>2013-02-05 18:35:10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-copOvldyKh1qr-vzkDEvw</td>\n",
       "      <td>MYoRNLb5chwjQe3c_k37Gg</td>\n",
       "      <td>It's open even when you think it isn't</td>\n",
       "      <td>2013-08-18 00:56:08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FjMQVZjSqY8syIO-53KFKw</td>\n",
       "      <td>hV-bABTK-glh5wj31ps_Jw</td>\n",
       "      <td>Very decent fried chicken</td>\n",
       "      <td>2017-06-27 23:05:38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ld0AperBXk1h6UbqmM80zw</td>\n",
       "      <td>_uN0OudeJ3Zl_tf6nxg5ww</td>\n",
       "      <td>Appetizers.. platter special for lunch</td>\n",
       "      <td>2012-10-06 19:43:09</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id             business_id  \\\n",
       "0  AGNUgVwnZUey3gcPCJ76iw  3uLgwr0qeCNMjKenHJwPGQ   \n",
       "1  NBN4MgHP9D3cw--SnauTkA  QoezRbYQncpRqyrLH6Iqjg   \n",
       "2  -copOvldyKh1qr-vzkDEvw  MYoRNLb5chwjQe3c_k37Gg   \n",
       "3  FjMQVZjSqY8syIO-53KFKw  hV-bABTK-glh5wj31ps_Jw   \n",
       "4  ld0AperBXk1h6UbqmM80zw  _uN0OudeJ3Zl_tf6nxg5ww   \n",
       "\n",
       "                                                text                date  \\\n",
       "0                     Avengers time with the ladies. 2012-05-18 02:17:21   \n",
       "1  They have lots of good deserts and tasty cuban... 2013-02-05 18:35:10   \n",
       "2             It's open even when you think it isn't 2013-08-18 00:56:08   \n",
       "3                          Very decent fried chicken 2017-06-27 23:05:38   \n",
       "4             Appetizers.. platter special for lunch 2012-10-06 19:43:09   \n",
       "\n",
       "   compliment_count  \n",
       "0                 0  \n",
       "1                 0  \n",
       "2                 0  \n",
       "3                 0  \n",
       "4                 0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_file_path = r\"E:\\DATA SCIENCE\\NLP-Tea\\Data\\yelp_academic_dataset_tip.json\\yelp_academic_dataset_tip.json\"\n",
    "df = pd.read_json(json_file_path, lines=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eeec26e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Avengers time with the ladies.',\n",
       " 'They have lots of good deserts and tasty cuban sandwiches',\n",
       " \"It's open even when you think it isn't\",\n",
       " 'Very decent fried chicken',\n",
       " 'Appetizers.. platter special for lunch']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data = list(df[\"text\"][:1000]) # First 1000 Row Only \n",
    "text_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806e1a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mohamad', 'fawzy', 'jfhbf', 'dvhbfehyv']\n"
     ]
    }
   ],
   "source": [
    "word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "5dc3e634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['avenger', 'time', 'lady'], ['lot', 'good', 'desert', 'tasty', 'cuban', 'sandwich'], ['open', 'even', 'think'], ['decent', 'fried', 'chicken'], ['appetizer', 'platter', 'special', 'lunch']]\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(text: str) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Preprocesses a given text:\n",
    "    - Lowercases text\n",
    "    - Contraction Handling\n",
    "    - Removes punctuation and digits\n",
    "    - Removes stopwords\n",
    "    - Tokenizes into words\n",
    "    - Applies lemmatization or stemming\n",
    "\n",
    "    Args:\n",
    "        document (str): The raw input text\n",
    "\n",
    "    Returns:\n",
    "        List of str: Cleaned and preprocessed text\n",
    "\n",
    "    Example:\n",
    "        >>> preprocessing(\"It's open even when you think it isn't\")\n",
    "        [\"'s\", 'open', 'even', 'think', \"n't\"]\n",
    "    \"\"\"\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Convert Text to Lowercase (Normalization)\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    # Contraction Handling\n",
    "    text_lower = contractions.fix(text_lower)\n",
    "\n",
    "    # Removing Punctuation\n",
    "    text_no_punct = re.sub(r'[^a-zA-Z\\s\\']', '', text_lower) # \\' for keep apostrophes (e.g. don't, it's)\n",
    "\n",
    "    # 3. Tokens\n",
    "    # tokens = word_tokenize(text_no_punct)\n",
    "    tokens = re.split(r\"\\s+\", text_no_punct)\n",
    "    \n",
    "\n",
    "    # 4. Stop word removal\n",
    "    filtered_tokens  = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # 5. Lemmatization \n",
    "    lemma_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens ]\n",
    "    \n",
    "\n",
    "    return lemma_tokens\n",
    "\n",
    "text_data = list(df[\"text\"][:10]) # First 1000 Row Only\n",
    "preprocessed_text = [preprocessing(text) for text in text_data]\n",
    "print(preprocessed_text[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "33d8ab7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['appetizer', 'area', 'avenger', 'best', 'boring', 'center', 'cheeseburger', 'chicken', 'chili', 'city', 'cocacolaso', 'cool', 'cuban', 'cup', 'dec', 'decent', 'decorated', 'desert', 'downtown', 'eat', 'elf', 'even', 'far', 'fried', 'game', 'good', 'great', 'kid', 'lady', 'leave', 'lindenwold', 'lot', 'lunch', 'make', 'never', 'onion', 'open', \"patco's\", 'pickle', 'place', 'platter', 'pm', 'probably', 'relish', 'ride', 'sandwich', 'santa', 'saturday', 'silver', 'single', 'sleigh', 'special', 'spring', 'starbucks', 'stop', 'substitute', 'taco', 'tampa', 'tasty', 'th', 'think', 'time', 'train', 'ugh', 'vanilla', 'w', 'watch']\n"
     ]
    }
   ],
   "source": [
    "all_tokens =[]\n",
    "for lst_tokens in preprocessed_text:\n",
    "    all_tokens.extend(lst_tokens)\n",
    "\n",
    "vocab = sorted(set(all_tokens)) # Unique Words\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c7f85b",
   "metadata": {},
   "source": [
    "# Label Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d94450f",
   "metadata": {},
   "source": [
    "## From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "7edba0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'appetizer': 0, 'area': 1, 'avenger': 2, 'best': 3, 'boring': 4, 'center': 5, 'cheeseburger': 6, 'chicken': 7, 'chili': 8, 'city': 9, 'cocacolaso': 10, 'cool': 11, 'cuban': 12, 'cup': 13, 'dec': 14, 'decent': 15, 'decorated': 16, 'desert': 17, 'downtown': 18, 'eat': 19, 'elf': 20, 'even': 21, 'far': 22, 'fried': 23, 'game': 24, 'good': 25, 'great': 26, 'kid': 27, 'lady': 28, 'leave': 29, 'lindenwold': 30, 'lot': 31, 'lunch': 32, 'make': 33, 'never': 34, 'onion': 35, 'open': 36, \"patco's\": 37, 'pickle': 38, 'place': 39, 'platter': 40, 'pm': 41, 'probably': 42, 'relish': 43, 'ride': 44, 'sandwich': 45, 'santa': 46, 'saturday': 47, 'silver': 48, 'single': 49, 'sleigh': 50, 'special': 51, 'spring': 52, 'starbucks': 53, 'stop': 54, 'substitute': 55, 'taco': 56, 'tampa': 57, 'tasty': 58, 'th': 59, 'think': 60, 'time': 61, 'train': 62, 'ugh': 63, 'vanilla': 64, 'w': 65, 'watch': 66}\n",
      "[[2, 61, 28], [31, 25, 17, 58, 12, 45], [36, 21, 60]]\n"
     ]
    }
   ],
   "source": [
    "def LabelEncoder(vocab: list) -> dict:\n",
    "    \"\"\"\n",
    "    Creates a label encoder that maps each unique word to a unique integer index.\n",
    "\n",
    "    Args:\n",
    "        vocab (list): A sorted list of unique words (vocabulary).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping words to their corresponding index.\n",
    "    \"\"\"\n",
    "    word_to_index = {token: idx for idx, token in enumerate(vocab)}\n",
    "    return word_to_index\n",
    "\n",
    "\n",
    "def Transform (preprocessed_text: list[list[str]], word_to_idx: dict) -> list[list[int]] :\n",
    "    \"\"\"\n",
    "    Transforms a list of tokenized text into lists of integer-encoded words.\n",
    "\n",
    "    Args:\n",
    "        preprocessed_text (list[list[str]]): A list of lists, where each sublist contains tokens from one sentence.\n",
    "        word_to_idx (dict): A dictionary mapping words to unique integer indices.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of lists, where each sublist contains the integer-encoded words for a sentence.\n",
    "    \"\"\"\n",
    "    data=[]\n",
    "    for sentence in preprocessed_text :\n",
    "        encoded_sentence = []\n",
    "        for word in sentence:\n",
    "            encoded_sentence.append(word_to_idx[word])\n",
    "        data.append(encoded_sentence)\n",
    "    return data\n",
    "\n",
    "\n",
    "word_to_idx = LabelEncoder(vocab= vocab)\n",
    "transformed_txt = Transform(preprocessed_text=preprocessed_text, word_to_idx=word_to_idx)\n",
    "print(word_to_idx)\n",
    "print(transformed_txt[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2d2df5",
   "metadata": {},
   "source": [
    "## Built in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "db04548e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded labels: [ 2 61 28 31 25 17 58 12 45 36 21 60 15 23  7  0 40 51 32  8 13 49  6 35\n",
      " 38 43 64 10 22 47 14 59 44 37 48 50 65 46 20 16 62  5  9 62 29 30 41 33\n",
      " 54 26 27 42  3 39 11 52  1 66 24 19 56 53 55  4 18 57 63 34]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "all_tokens =[]\n",
    "for lst_tokens in preprocessed_text:\n",
    "    all_tokens.extend(lst_tokens) # All Words\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(all_tokens)\n",
    "\n",
    "# Output encoded labels and the mapping\n",
    "print(\"Encoded labels:\", encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "105c2030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 2, 61, 28]),\n",
       " array([31, 25, 17, 58, 12, 45]),\n",
       " array([36, 21, 60]),\n",
       " array([15, 23,  7]),\n",
       " array([ 0, 40, 51, 32]),\n",
       " array([ 8, 13, 49,  6, 35, 38, 43, 64, 10, 22]),\n",
       " array([47, 14, 59, 44, 37, 48, 50, 65, 46, 20, 16, 62,  5,  9, 62, 29, 30,\n",
       "        41, 33, 54, 26, 27]),\n",
       " array([42,  3, 39, 11, 52,  1, 66, 24, 19]),\n",
       " array([56]),\n",
       " array([53, 55,  4, 18, 57, 63, 34])]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sentences = [label_encoder.transform(sentence) for sentence in preprocessed_text]\n",
    "encoded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f03b91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efff3800",
   "metadata": {},
   "source": [
    "# One hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "f9f691ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "       0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0])]\n"
     ]
    }
   ],
   "source": [
    "def OneHotEncoder(vocab: list) -> dict :\n",
    "    \"\"\"\n",
    "    Creates one-hot encoded vectors for each unique word in the vocabulary.\n",
    "\n",
    "    Args:\n",
    "        vocab (list): A sorted list of unique tokens.\n",
    "\n",
    "    Returns:\n",
    "        dict: A mapping from word to its one-hot encoded numpy array.\n",
    "    \"\"\"\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    vocab_size = len(word_to_idx)\n",
    "    one_hot_dict  = {}\n",
    "\n",
    "    for word, idx in word_to_idx.items() :\n",
    "        # print(word, idx)\n",
    "        vec = np.zeros(shape=vocab_size, dtype=int)\n",
    "        vec[idx] = 1\n",
    "        one_hot_dict[word] = vec\n",
    "\n",
    "    return one_hot_dict\n",
    "\n",
    "\n",
    "def TransformOneHot(preprocessed_text: list[list[str]], word_to_vec: dict) -> list[list[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Transforms a list of tokenized sentences into one-hot encoded vectors.\n",
    "\n",
    "    Args:\n",
    "        preprocessed_text (list of list of str): Tokenized sentences.\n",
    "        word_to_vec (dict): A mapping from word to one-hot vector.\n",
    "\n",
    "    Returns:\n",
    "        list of list of np.ndarray: One-hot encoded representation of sentences.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for sentence in preprocessed_text:\n",
    "        encoded_sentence = []\n",
    "        for word in sentence:\n",
    "            encoded_sentence.append(word_to_vec[word])\n",
    "        data.append(encoded_sentence)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "one_hot_dict  = OneHotEncoder(vocab)\n",
    "transformed_txt = TransformOneHot(preprocessed_text=preprocessed_text, word_to_vec=one_hot_dict)\n",
    "# print(one_hot_dict)\n",
    "print(transformed_txt[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa185195",
   "metadata": {},
   "source": [
    "## Built in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "44c34f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "9d58a0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n",
      "67\n",
      "{'appetizer': 0, 'area': 1, 'avenger': 2, 'best': 3, 'boring': 4, 'center': 5, 'cheeseburger': 6, 'chicken': 7, 'chili': 8, 'city': 9, 'cocacolaso': 10, 'cool': 11, 'cuban': 12, 'cup': 13, 'dec': 14, 'decent': 15, 'decorated': 16, 'desert': 17, 'downtown': 18, 'eat': 19, 'elf': 20, 'even': 21, 'far': 22, 'fried': 23, 'game': 24, 'good': 25, 'great': 26, 'kid': 27, 'lady': 28, 'leave': 29, 'lindenwold': 30, 'lot': 31, 'lunch': 32, 'make': 33, 'never': 34, 'onion': 35, 'open': 36, \"patco's\": 37, 'pickle': 38, 'place': 39, 'platter': 40, 'pm': 41, 'probably': 42, 'relish': 43, 'ride': 44, 'sandwich': 45, 'santa': 46, 'saturday': 47, 'silver': 48, 'single': 49, 'sleigh': 50, 'special': 51, 'spring': 52, 'starbucks': 53, 'stop': 54, 'substitute': 55, 'taco': 56, 'tampa': 57, 'tasty': 58, 'th': 59, 'think': 60, 'time': 61, 'train': 62, 'ugh': 63, 'vanilla': 64, 'w': 65, 'watch': 66}\n"
     ]
    }
   ],
   "source": [
    "all_tokens =[]\n",
    "for lst_tokens in preprocessed_text:\n",
    "    all_tokens.extend(lst_tokens) # All Words\n",
    "vocab = sorted(set(all_tokens)) # Unique Words\n",
    "\n",
    "print(len(all_tokens))\n",
    "print(len(vocab))\n",
    "w_idx = {w:i for i,w in enumerate(vocab)}\n",
    "print(w_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "0c7cffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(array(vocab)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "9b16dccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'appetizer': 0, 'area': 1, 'avenger': 2, 'best': 3, 'boring': 4, 'center': 5, 'cheeseburger': 6, 'chicken': 7, 'chili': 8, 'city': 9, 'cocacolaso': 10, 'cool': 11, 'cuban': 12, 'cup': 13, 'dec': 14, 'decent': 15, 'decorated': 16, 'desert': 17, 'downtown': 18, 'eat': 19, 'elf': 20, 'even': 21, 'far': 22, 'fried': 23, 'game': 24, 'good': 25, 'great': 26, 'kid': 27, 'lady': 28, 'leave': 29, 'lindenwold': 30, 'lot': 31, 'lunch': 32, 'make': 33, 'never': 34, 'onion': 35, 'open': 36, \"patco's\": 37, 'pickle': 38, 'place': 39, 'platter': 40, 'pm': 41, 'probably': 42, 'relish': 43, 'ride': 44, 'sandwich': 45, 'santa': 46, 'saturday': 47, 'silver': 48, 'single': 49, 'sleigh': 50, 'special': 51, 'spring': 52, 'starbucks': 53, 'stop': 54, 'substitute': 55, 'taco': 56, 'tampa': 57, 'tasty': 58, 'th': 59, 'think': 60, 'time': 61, 'train': 62, 'ugh': 63, 'vanilla': 64, 'w': 65, 'watch': 66}\n",
      "{'appetizer': 0, 'area': 1, 'avenger': 2, 'best': 3, 'boring': 4, 'center': 5, 'cheeseburger': 6, 'chicken': 7, 'chili': 8, 'city': 9, 'cocacolaso': 10, 'cool': 11, 'cuban': 12, 'cup': 13, 'dec': 14, 'decent': 15, 'decorated': 16, 'desert': 17, 'downtown': 18, 'eat': 19, 'elf': 20, 'even': 21, 'far': 22, 'fried': 23, 'game': 24, 'good': 25, 'great': 26, 'kid': 27, 'lady': 28, 'leave': 29, 'lindenwold': 30, 'lot': 31, 'lunch': 32, 'make': 33, 'never': 34, 'onion': 35, 'open': 36, \"patco's\": 37, 'pickle': 38, 'place': 39, 'platter': 40, 'pm': 41, 'probably': 42, 'relish': 43, 'ride': 44, 'sandwich': 45, 'santa': 46, 'saturday': 47, 'silver': 48, 'single': 49, 'sleigh': 50, 'special': 51, 'spring': 52, 'starbucks': 53, 'stop': 54, 'substitute': 55, 'taco': 56, 'tampa': 57, 'tasty': 58, 'th': 59, 'think': 60, 'time': 61, 'train': 62, 'ugh': 63, 'vanilla': 64, 'w': 65, 'watch': 66}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([2, 61, 28], [2, 61, 28])"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(integer_encoded)\n",
    "# print(vocab)\n",
    "# print(preprocessed_text )\n",
    "word2id = dict(zip(vocab, integer_encoded))\n",
    "print(word2id)\n",
    "print(w_idx)\n",
    "\n",
    "datamodel=[]\n",
    "data_me=[]\n",
    "\n",
    "for sentence in preprocessed_text:\n",
    "    lmodel=[]\n",
    "    lme=[]\n",
    "\n",
    "    for w in sentence:\n",
    "        lmodel.append(word2id[w])\n",
    "        lme.append(w_idx[w])\n",
    "    datamodel.append(lmodel)\n",
    "    data_me.append(lme)\n",
    "#--------------------------------------------*************----------------\n",
    "data_me[0], datamodel[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4633d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "42bbccf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array, reshape\n",
    "\n",
    "integer_encoded = integer_encoded.reshape(-1, 1)\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "b78d1f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]\n"
     ]
    }
   ],
   "source": [
    "word2onehot = dict(zip(vocab, onehot_encoded))\n",
    "\n",
    "data=[]\n",
    "for sentence in preprocessed_text:\n",
    "    vec = []\n",
    "    for w in sentence:\n",
    "        vec.append(word2onehot[w])\n",
    "    data.append(vec)\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3dc3b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
