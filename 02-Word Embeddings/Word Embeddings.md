# Word Embeddings

## What is Word Embedding ?

Inputs to Machine learning algorithms are Numbers (Scalars, Vectors). <br>
Text must be converted into vectors.<br>

a way of representing words as vectors in a multi-dimensional space, where the distance between vectors reflect the similarity and relationships between the words.<br>

representing words in a way that machines can understand. <br>

There are two main Approaches for word embedding:
-  Frequency Based Embedding
    -  [Label (integer) Encoding](https://github.com/Fawzy-AI-Explorer/NLP-Tea/tree/main/02-Word%20Embeddings/2.1-Label%20Encoder%20and%20One%20Hot%20Encoder)  
    -  [One-Hot encoded vector](https://github.com/Fawzy-AI-Explorer/NLP-Tea/tree/main/02-Word%20Embeddings/2.1-Label%20Encoder%20and%20One%20Hot%20Encoder) 
    -  [Bag of Word (BOW) Count Vector](https://github.com/Fawzy-AI-Explorer/NLP-Tea/tree/main/02-Word%20Embeddings/2.2-BOW) 
    -  [Term Frequency- Inverse Document frequency (TF-IDF) Vector](https://github.com/Fawzy-AI-Explorer/NLP-Tea/tree/main/02-Word%20Embeddings/2.3-TF_IDF)  
-  Prediction Based Embedding
    -  Word2Vec
        -  CBOW
        -  Skip Gram
            -  Negative Sampling  
    -  Fast Text    



 





