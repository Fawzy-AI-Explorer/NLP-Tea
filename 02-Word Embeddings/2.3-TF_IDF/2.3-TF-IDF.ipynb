{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30fa1fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\htc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\htc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import contractions\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6dcf6c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"Neural networks process data using deep learning algorithms in artificial intelligence.\"\n",
    "doc2 = \"Artificial intelligence applies neural networks and deep learning to process large datasets.\"\n",
    "\n",
    "doc3 = \"Gasoline cars have combustion engines that power vehicles through fuel ignition.\"\n",
    "doc4 = \"Car engines burn gasoline in combustion chambers to move vehicles on the road.\"\n",
    "\n",
    "corpus = [doc1, doc2, doc3, doc4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f4af634d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural networks process data using deep learning algorithms in artificial intelligence.\n",
      "Length of document: 87\n",
      "\n",
      "Artificial intelligence applies neural networks and deep learning to process large datasets.\n",
      "Length of document: 92\n",
      "\n",
      "Gasoline cars have combustion engines that power vehicles through fuel ignition.\n",
      "Length of document: 80\n",
      "\n",
      "Car engines burn gasoline in combustion chambers to move vehicles on the road.\n",
      "Length of document: 78\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for d in corpus:\n",
    "    print(d)\n",
    "    print(\"Length of document:\", len(d))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c96b80e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['neural', 'network', 'process', 'data', 'using', 'deep', 'learning', 'algorithm', 'artificial', 'intelligence'], ['artificial', 'intelligence', 'applies', 'neural', 'network', 'deep', 'learning', 'process', 'large', 'datasets'], ['gasoline', 'car', 'combustion', 'engine', 'power', 'vehicle', 'fuel', 'ignition'], ['car', 'engine', 'burn', 'gasoline', 'combustion', 'chamber', 'move', 'vehicle', 'road']]\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(text: str) -> list[str]:\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # Convert Text to Lowercase (Normalization)\n",
    "    text_lower = text.lower()\n",
    "    text_no_tags = re.sub(r'<[^>]+>', '', text_lower)\n",
    "\n",
    "    # Contraction Handling\n",
    "    text_no_tags = contractions.fix(text_no_tags)\n",
    "\n",
    "    # Removing Punctuation\n",
    "    text_no_punct = re.sub(r'[^a-zA-Z\\s]', '', text_no_tags) # \\' for keep apostrophes (e.g. don't, it's)\n",
    "\n",
    "\n",
    "    # 3. Tokens\n",
    "    tokens = re.split(r\"\\s+\", text_no_punct) \n",
    "    tokens = [t for t in tokens if t]\n",
    "    # or use nltk tokenizer\n",
    "    tokens = word_tokenize(text_no_punct)\n",
    "\n",
    "    # 4. Stop word removal\n",
    "    filtered_tokens  = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # 5. Lemmatization \n",
    "    lemma_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens ]\n",
    "    # or stemmer\n",
    "    stemm_tokens = [stemmer.stem(token) for token in filtered_tokens ]\n",
    "\n",
    "    return lemma_tokens\n",
    "\n",
    "preprocessed_text = [preprocessing(doc) for doc in corpus]\n",
    "print(preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "91ad8dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term : learning\n",
      "doc :['neural', 'network', 'process', 'data', 'using', 'deep', 'learning', 'algorithm', 'artificial', 'intelligence']\n",
      "frequency : 1\n",
      "len doc : 10\n",
      "tf of 'learning' on doc 0:  0.1\n",
      "-----------------------\n",
      "term : learning\n",
      "doc :['artificial', 'intelligence', 'applies', 'neural', 'network', 'deep', 'learning', 'process', 'large', 'datasets']\n",
      "frequency : 1\n",
      "len doc : 10\n",
      "tf of 'learning' on doc 2:  0.1\n"
     ]
    }
   ],
   "source": [
    "all_tokens = [token for doc in preprocessed_text for token in doc]\n",
    "vocab = sorted(set(all_tokens))\n",
    "# print(len(all_tokens))\n",
    "\n",
    "\n",
    "def TF(term, doc) :\n",
    "    term = term.lower()\n",
    "    print(f\"term : {term}\")\n",
    "    print(f\"doc :{doc}\")\n",
    "    print(f\"frequency : {doc.count(term)}\")\n",
    "    print(f\"len doc : {len(doc)}\")\n",
    "    return doc.count(term) / len(doc)\n",
    "\n",
    "\n",
    "term = \"learning\"\n",
    "tf = TF(term, preprocessed_text[0])\n",
    "print(\"tf of 'learning' on doc 0: \", tf)\n",
    "print(\"-----------------------\")\n",
    "tf = TF(term, preprocessed_text[1])\n",
    "print(\"tf of 'learning' on doc 2: \", tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ea9c103f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term : learning \n",
      "number of documents : 4 \n",
      "number of documents containing term : 2\n",
      "idf of 'learning' :  1.3333333333333333\n",
      "-----------------------\n",
      "term : statistic \n",
      "number of documents : 4 \n",
      "number of documents containing term : 0\n",
      "idf of 'statistics' :  4.0\n"
     ]
    }
   ],
   "source": [
    "def IDF(term, corpus):\n",
    "    term = term.lower()\n",
    "    N = len(corpus)\n",
    "    n = sum(1 for doc in corpus if term in doc)\n",
    "    print(f\"term : {term} \\nnumber of documents : {N} \\nnumber of documents containing term : {n}\")\n",
    "\n",
    "    return N/(n+1)\n",
    "idf = IDF(\"learning\", preprocessed_text)\n",
    "print(\"idf of 'learning' : \", idf)\n",
    "print(\"-----------------------\")\n",
    "idf = IDF(\"statistic\", preprocessed_text)\n",
    "print(\"idf of 'statistics' : \", idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b9d49d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 25)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens = [token for doc in preprocessed_text for token in doc]\n",
    "vocab = sorted(set(all_tokens))\n",
    "print(len(all_tokens))\n",
    "\n",
    "\n",
    "def TF(term: str, doc: list[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Term Frequency (TF) of a term in a document.\n",
    "\n",
    "    Args:\n",
    "        term (str): The term to calculate TF for.\n",
    "        doc (list[str]): The document in which to calculate TF.\n",
    "\n",
    "    Returns:\n",
    "        float: The term frequency of the term in the document.\n",
    "    \"\"\"\n",
    "    term = term.lower()\n",
    "    return doc.count(term) / len(doc)\n",
    "    \n",
    "def IDF(term: str, corpus: List[list[str]]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Inverse Document Frequency (IDF) of a term in a corpus.\n",
    "\n",
    "    Args:\n",
    "        term (str): The term to calculate IDF for.\n",
    "        corpus (List[list[str]]): The corpus in which to calculate IDF.\n",
    "\n",
    "    Returns:\n",
    "        float: The inverse document frequency of the term in the corpus.\n",
    "    \"\"\"\n",
    "    N = len(corpus)\n",
    "    term = term.lower()\n",
    "    num_docs_with_term = sum(1 for doc in corpus if term in doc)\n",
    "    return N / (1 + num_docs_with_term)\n",
    "\n",
    "def TF_IDF(term: str, doc: list[str], corpus: List[list[str]]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate TF-IDF of a term in a document within a corpus.\n",
    "\n",
    "    Args:\n",
    "        term (str): The term to calculate TF-IDF for.\n",
    "        doc (list[str]): The document in which to calculate TF-IDF.\n",
    "        corpus (List[list[str]]): The corpus in which to calculate TF-IDF.\n",
    "\n",
    "    Returns:\n",
    "        float: The TF-IDF score of the term in the document.\n",
    "    \"\"\"\n",
    "    tf = TF(term, doc)\n",
    "    idf = IDF(term, corpus)\n",
    "    return tf * idf\n",
    "\n",
    "\n",
    "\n",
    "tfidf_matrix = np.zeros((len(preprocessed_text), len(vocab)))\n",
    "for i, doc in enumerate(preprocessed_text):\n",
    "    for j, term in enumerate(vocab):\n",
    "        tfidf_matrix[i][j] = TF_IDF(term, doc, preprocessed_text)\n",
    "\n",
    "\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6afe47a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.50909091 0.         0.        ]\n",
      " [0.50909091 1.         0.         0.        ]\n",
      " [0.         0.         1.         0.38984059]\n",
      " [0.         0.         0.38984059 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# calc similarity between documents\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4239f7c3",
   "metadata": {},
   "source": [
    "# Built in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0459ffb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['advanced' 'ai' 'algorithms' 'allows' 'artificial' 'benefits' 'branch'\n",
      " 'cars' 'data' 'decisions' 'designed' 'efficiency' 'electric'\n",
      " 'environmental' 'features' 'include' 'intelligence' 'learn' 'learning'\n",
      " 'machine' 'machines' 'make' 'patterns' 'popular' 'safety'\n",
      " 'transportation' 'uses' 'vehicles']\n",
      "--------------\n",
      "[[0.         0.         0.         0.36222393 0.36222393 0.\n",
      "  0.         0.         0.2855815  0.36222393 0.         0.\n",
      "  0.         0.         0.         0.         0.36222393 0.36222393\n",
      "  0.         0.         0.36222393 0.36222393 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.36222393 0.36222393 0.         0.         0.\n",
      "  0.36222393 0.         0.2855815  0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.36222393 0.36222393 0.         0.         0.36222393 0.\n",
      "  0.         0.         0.36222393 0.        ]\n",
      " [0.37796447 0.         0.         0.         0.         0.\n",
      "  0.         0.37796447 0.         0.         0.37796447 0.\n",
      "  0.         0.         0.37796447 0.37796447 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.37796447 0.37796447 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.40824829\n",
      "  0.         0.         0.         0.         0.         0.40824829\n",
      "  0.40824829 0.40824829 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.40824829\n",
      "  0.         0.         0.         0.40824829]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "documents = [doc1, doc2, doc3, doc4]\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(\"--------------\")\n",
    "# Convert TF-IDF matrix to array and view it\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5ee47d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
