{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03384981",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5baf0687",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\htc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\htc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89132fd3",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f1a8e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>compliment_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AGNUgVwnZUey3gcPCJ76iw</td>\n",
       "      <td>3uLgwr0qeCNMjKenHJwPGQ</td>\n",
       "      <td>Avengers time with the ladies.</td>\n",
       "      <td>2012-05-18 02:17:21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NBN4MgHP9D3cw--SnauTkA</td>\n",
       "      <td>QoezRbYQncpRqyrLH6Iqjg</td>\n",
       "      <td>They have lots of good deserts and tasty cuban...</td>\n",
       "      <td>2013-02-05 18:35:10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-copOvldyKh1qr-vzkDEvw</td>\n",
       "      <td>MYoRNLb5chwjQe3c_k37Gg</td>\n",
       "      <td>It's open even when you think it isn't</td>\n",
       "      <td>2013-08-18 00:56:08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FjMQVZjSqY8syIO-53KFKw</td>\n",
       "      <td>hV-bABTK-glh5wj31ps_Jw</td>\n",
       "      <td>Very decent fried chicken</td>\n",
       "      <td>2017-06-27 23:05:38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ld0AperBXk1h6UbqmM80zw</td>\n",
       "      <td>_uN0OudeJ3Zl_tf6nxg5ww</td>\n",
       "      <td>Appetizers.. platter special for lunch</td>\n",
       "      <td>2012-10-06 19:43:09</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id             business_id  \\\n",
       "0  AGNUgVwnZUey3gcPCJ76iw  3uLgwr0qeCNMjKenHJwPGQ   \n",
       "1  NBN4MgHP9D3cw--SnauTkA  QoezRbYQncpRqyrLH6Iqjg   \n",
       "2  -copOvldyKh1qr-vzkDEvw  MYoRNLb5chwjQe3c_k37Gg   \n",
       "3  FjMQVZjSqY8syIO-53KFKw  hV-bABTK-glh5wj31ps_Jw   \n",
       "4  ld0AperBXk1h6UbqmM80zw  _uN0OudeJ3Zl_tf6nxg5ww   \n",
       "\n",
       "                                                text                date  \\\n",
       "0                     Avengers time with the ladies. 2012-05-18 02:17:21   \n",
       "1  They have lots of good deserts and tasty cuban... 2013-02-05 18:35:10   \n",
       "2             It's open even when you think it isn't 2013-08-18 00:56:08   \n",
       "3                          Very decent fried chicken 2017-06-27 23:05:38   \n",
       "4             Appetizers.. platter special for lunch 2012-10-06 19:43:09   \n",
       "\n",
       "   compliment_count  \n",
       "0                 0  \n",
       "1                 0  \n",
       "2                 0  \n",
       "3                 0  \n",
       "4                 0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_file_path = r\"E:\\DATA SCIENCE\\NLP-Tea\\Data\\yelp_academic_dataset_tip.json\\yelp_academic_dataset_tip.json\"\n",
    "df = pd.read_json(json_file_path, lines=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e99b254f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(908915, 5)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8dfe82f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Avengers time with the ladies.',\n",
       " 'They have lots of good deserts and tasty cuban sandwiches',\n",
       " \"It's open even when you think it isn't\",\n",
       " 'Very decent fried chicken',\n",
       " 'Appetizers.. platter special for lunch']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data = list(df[\"text\"][:1000]) # First 1000 Row Only \n",
    "text_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf06943",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "02269239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self serve onions, relish, mayo?  And FREE caramelized onions?  Yes!\n"
     ]
    }
   ],
   "source": [
    "test_text = text_data[101]\n",
    "print(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13434b4",
   "metadata": {},
   "source": [
    "## Case Normalization (lowercase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "29303df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text : Self serve onions, relish, mayo?  And FREE caramelized onions?  Yes!\n",
      "lowercase text: self serve onions, relish, mayo?  and free caramelized onions?  yes!\n"
     ]
    }
   ],
   "source": [
    "text_lower = test_text.lower()\n",
    "print(f\"original text : {test_text}\")\n",
    "print(f\"lowercase text: {text_lower}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d864693",
   "metadata": {},
   "source": [
    "## Removes punctuation and digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1b2e7d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text : Self serve onions, relish, mayo?  And FREE caramelized onions?  Yes!\n",
      "preprocessed  : self serve onions relish mayo  and free caramelized onions  yes\n"
     ]
    }
   ],
   "source": [
    "text_lower = test_text.lower()\n",
    "text_no_punct = re.sub(r'[^a-zA-z\\s]', '', text_lower) # keep only letters and space\n",
    "text_no_punct = re.sub(r'[^a-zA-z\\s0-9]', '', text_lower) # Keep numbers \n",
    "\n",
    "\n",
    "print(f\"original text : {test_text}\")\n",
    "print(f\"preprocessed  : {text_no_punct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9f6006e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text : don't\n",
      "with \\'        : don't\n",
      "with out \\'    : dont\n"
     ]
    }
   ],
   "source": [
    "text_no_punct1 = re.sub(r'[^a-zA-z\\s\\']', '', \"don't\")\n",
    "text_no_punct2 = re.sub(r'[^a-zA-z\\s]', '', \"don't\")\n",
    "\n",
    "print(f\"original text : don't\")\n",
    "print(f\"with \\\\'        : {text_no_punct1}\")\n",
    "print(f\"with out \\\\'    : {text_no_punct2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ec81bf",
   "metadata": {},
   "source": [
    "## Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a727391a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text : Self serve onions, relish, mayo?  And FREE caramelized onions?  Yes!\n",
      "preprocessed  : ['self', 'serve', 'onions', 'relish', 'mayo', 'and', 'free', 'caramelized', 'onions', 'yes']\n",
      "preprocessed_1: ['self', 'serve', 'onions', 'relish', 'mayo', 'and', 'free', 'caramelized', 'onions', 'yes']\n"
     ]
    }
   ],
   "source": [
    "text_lower = test_text.lower()\n",
    "text_no_punct = re.sub(r'[^a-zA-z\\s]', '', text_lower) \n",
    "tokens = re.split(r\"\\s+\", text_no_punct) \n",
    "\n",
    "# or \n",
    "tokens_v1 = word_tokenize(text_no_punct)\n",
    "\n",
    "print(f\"original text : {test_text}\")\n",
    "print(f\"preprocessed  : {tokens}\")\n",
    "print(f\"preprocessed_1: {tokens_v1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f69d1da",
   "metadata": {},
   "source": [
    "## Removes stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b95e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text : Self serve onions, relish, mayo?  And FREE caramelized onions?  Yes!\n",
      "preprocessed  : ['self', 'serve', 'onions', 'relish', 'mayo', 'free', 'caramelized', 'onions', 'yes']\n"
     ]
    }
   ],
   "source": [
    "text_lower = test_text.lower()\n",
    "text_no_punct = re.sub(r'[^a-zA-z\\s]', '', text_lower)\n",
    "tokens = re.split(r\"\\s+\", text_no_punct) \n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "\n",
    "\n",
    "print(f\"original text : {test_text}\")\n",
    "print(f\"preprocessed  : {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d221f8d2",
   "metadata": {},
   "source": [
    "## Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822ea890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text : Self serve onions, relish, mayo?  And FREE caramelized onions?  Yes!\n",
      "preprocessed  : ['self', 'serv', 'onion', 'relish', 'mayo', 'free', 'caramel', 'onion', 'ye']\n"
     ]
    }
   ],
   "source": [
    "# Initialize stemmer \n",
    "stemmer = PorterStemmer()\n",
    "stem_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "print(f\"original text : {test_text}\")\n",
    "print(f\"preprocessed  : {stem_tokens}\")\n",
    "\n",
    "#server =>> serv\n",
    "# yes =>> ye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eb5608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text : Self serve onions, relish, mayo?  And FREE caramelized onions?  Yes!\n",
      "preprocessed  : ['self', 'serve', 'onion', 'relish', 'mayo', 'free', 'caramelized', 'onion', 'yes']\n"
     ]
    }
   ],
   "source": [
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemma_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "print(f\"original text : {test_text}\")\n",
    "print(f\"preprocessed  : {lemma_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7d25b322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: running\n",
      "  Stemmed:     run\n",
      "  Lemmatized:  running\n",
      "\n",
      "Word: better\n",
      "  Stemmed:     better\n",
      "  Lemmatized:  better\n",
      "\n",
      "Word: flies\n",
      "  Stemmed:     fli\n",
      "  Lemmatized:  fly\n",
      "\n",
      "Word: cities\n",
      "  Stemmed:     citi\n",
      "  Lemmatized:  city\n",
      "\n",
      "Word: served\n",
      "  Stemmed:     serv\n",
      "  Lemmatized:  served\n",
      "\n",
      "Word: children\n",
      "  Stemmed:     children\n",
      "  Lemmatized:  child\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words = [\"running\", \"better\", \"flies\", \"cities\", \"served\", \"children\"]\n",
    "\n",
    "for word in words:\n",
    "    print(f\"Word: {word}\")\n",
    "    print(f\"  Stemmed:     {stemmer.stem(word)}\")\n",
    "    print(f\"  Lemmatized:  {lemmatizer.lemmatize(word)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf159a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a274ddb0",
   "metadata": {},
   "source": [
    "## ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ed33f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import nltk\n",
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685cadf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['avenger', 'time', 'lady'], ['lot', 'good', 'desert', 'tasty', 'cuban', 'sandwich'], [\"'s\", 'open', 'even', 'think', \"n't\"], ['decent', 'fried', 'chicken'], ['appetizer', 'platter', 'special', 'lunch']]\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Preprocesses a given text:\n",
    "    - Lowercases text\n",
    "    - Removes punctuation and digits\n",
    "    - Removes stopwords\n",
    "    - Tokenizes into words\n",
    "    - Applies lemmatization or stemming\n",
    "\n",
    "    Args:\n",
    "        document (str): The raw input text\n",
    "\n",
    "    Returns:\n",
    "        List of str: Cleaned and preprocessed text\n",
    "\n",
    "    Example:\n",
    "        >>> preprocess(\"I love Python! üòä It's awesome üëç\")\n",
    "        ['love', 'python', 'smiling_face', 'awesome', 'thumbs_up']\n",
    "    \"\"\"\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # Convert Text to Lowercase (Normalization)\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    # Removing Punctuation\n",
    "    text_no_punct = re.sub(r'[^a-zA-Z\\s\\']', '', text_lower) # \\' for keep apostrophes (e.g. don't, it's)\n",
    "\n",
    "\n",
    "    # 3. Tokens\n",
    "    tokens = re.split(r\"\\s+\", text_no_punct) \n",
    "    tokens = [t for t in tokens if t]\n",
    "    # or use nltk tokenizer\n",
    "    tokens = word_tokenize(text_no_punct)\n",
    "\n",
    "    # 4. Stop word removal\n",
    "    filtered_tokens  = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # 5. Lemmatization \n",
    "    lemma_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens ]\n",
    "    # or stemmer\n",
    "    stemm_tokens = [stemmer.stem(token) for token in filtered_tokens ]\n",
    "\n",
    "    return lemma_tokens\n",
    "\n",
    "text_data = list(df[\"text\"][:100]) # First 1000 Row Only\n",
    "preprocessed_text = [preprocessing(text) for text in text_data]\n",
    "print(preprocessed_text[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1016ea24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love  pizza  and  grinning face with smiling eyes !'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(preprocessing(\"I love Python! üòä It's awesome üëç\"))\n",
    "text = emoji.demojize(\"I love üçï and üòÑ!\", delimiters=(\" \", \" \")) \n",
    "text = re.sub(r'_', ' ', text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8531a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Remove URLs, emails, and Twitter mentions\n",
    "text = re.sub(r'(https?://\\S+|www\\.\\S+)', ' ', text)   # URLs\n",
    "text = re.sub(r'\\S+@\\S+', ' ', text)                    # Email addresses\n",
    "text = re.sub(r'@\\w+', ' ', text)                       # Mentions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
